{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report #for detailed statistics on classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.datasets import MNIST  #importing MNIST dataset\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms #for transforming the training and testing data \n",
    "from torch.utils.data import DataLoader #Dataloader loads the data batchwise with shuffling in a hassle free manner\n",
    "from torch.optim import Adam #Adam for GD\n",
    "import time # to see how long training took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags for running various parts of the assignment\n",
    "\n",
    "download_flag = False\n",
    "load_model    = False\n",
    "master_dir = os.getcwd() #this is the directory we're working in\n",
    "mnist_dir     = 'mnist_A2' #directory to store the MNIST dataset\n",
    "model_dir     = 'RNN_model' #directory containing the AE model\n",
    "download_dir  = 'C:\\\\Users\\\\ABHISHEK\\\\Downloads\\\\EE6132_Ass4'\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(master_dir, mnist_dir)):\n",
    "        os.mkdir(os.path.join(master_dir, mnist_dir)) #make the directory if it doesn't exist\n",
    "\n",
    "if not os.path.exists(os.path.join(master_dir, model_dir)):\n",
    "        os.mkdir(os.path.join(master_dir, model_dir)) #make the directory if it doesn't exist\n",
    "model_path = os.path.join(master_dir, model_dir)+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Hyperparams:\n",
    "learning_rate = 1e-2 \n",
    "batch_size    = 64\n",
    "N_epochs      = 7 #make it 5 for image, 30 for seq, 7 for binary\n",
    "N_iter_train  = 250\n",
    "N_iter_test   = 40\n",
    "N_iter_check  = 5\n",
    "\n",
    "input_size    = 10 #MNIST inputs sent as 28 units of 28x1 vectors, 10 for the second question\n",
    "N_steps       = 28 #no of time steps\n",
    "\n",
    "\n",
    "#device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks for gpu else runs in cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size = input_size,hidden = 128,bd_flag = False): #class constructor with params for hidden layer and input size\n",
    "        super(Vanilla_RNN, self).__init__() #calls the parent constructor\n",
    "        \n",
    "        #configuring the RNN\n",
    "        self.rnn = nn.RNN(input_size = input_size, \n",
    "                          hidden_size = hidden, \n",
    "                          num_layers = 1,\n",
    "                          bidirectional = bd_flag, \n",
    "                          batch_first = True)\n",
    "        \n",
    "        #we want to use the output of the Hidden Layer for the next time step\n",
    "        self.HL = nn.Linear(hidden + hidden*bd_flag ,10) #as size of the output is 10\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self,x): #defines the forward pass and also the structure of the network thus helping backprop\n",
    "        \n",
    "        # x :(batch_size, #time_steps, input_size)\n",
    "        # out:(batch_size, #time_steps, output_size)\n",
    "        \n",
    "        out,hidden_ = self.rnn(x)\n",
    "        \n",
    "        #we want the output at the last time step alone\n",
    "        out = self.HL(out[:,-1,:]) #obtain the output of the last hidden state\n",
    "        \n",
    "        pred   = self.logsoftmax(out)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "class Vanilla_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size = input_size,hidden = 128,bd_flag = False): #class constructor with params for hidden layer and input size\n",
    "        super(Vanilla_LSTM, self).__init__() #calls the parent constructor\n",
    "        \n",
    "        #configuring the RNN\n",
    "        self.lstm = nn.LSTM(input_size = input_size, \n",
    "                          hidden_size = hidden, \n",
    "                          num_layers = 1,\n",
    "                          bidirectional = bd_flag, \n",
    "                          batch_first = True)\n",
    "        \n",
    "        #we want to use the output of the Hidden Layer for the next time step\n",
    "        self.HL = nn.Linear(hidden + hidden*bd_flag,10) #as size of the output is 10\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "        self.bd_flag = bd_flag\n",
    "        self.hidden  = hidden\n",
    "        \n",
    "    \n",
    "    def forward(self,x): #defines the forward pass and also the structure of the network thus helping backprop\n",
    "        \n",
    "        # x :(batch_size, #time_steps, input_size)\n",
    "        # out:(batch_size, #time_steps, output_size)\n",
    "        # h :(D*#hidden_layers, batch_size, hidden_size)\n",
    "        # c :(D*#hidden_layers, batch_size, hidden_size)\n",
    "        # D = 2 if bidirectional else 1 \n",
    "        \n",
    "        #initializing the cell and hidden state to all zeros for the first input\n",
    "        \n",
    "        h_0 = torch.zeros(1 + 1*self.bd_flag, x.size(0), self.hidden).requires_grad_()\n",
    "\n",
    "        c_0 = torch.zeros(1 + 1*self.bd_flag, x.size(0), self.hidden).requires_grad_()\n",
    "\n",
    "        # We detach as we're doing truncated BPTT and don't wanna start from the beginning for a new batch\n",
    "        \n",
    "        # Forward prop\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0.detach(), c_0.detach()))\n",
    "\n",
    "        #we want the output at the last time step alone\n",
    "        out = self.HL(out[:,-1,:]) #obtain the output of the last hidden state\n",
    "\n",
    "        pred   = self.logsoftmax(out)\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "        \n",
    "class Vanilla_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size = input_size,hidden = 128,bd_flag = False): #class constructor with params for hidden layer and input size\n",
    "        super(Vanilla_GRU, self).__init__() #calls the parent constructor\n",
    "        \n",
    "        #configuring the RNN\n",
    "        self.gru = nn.GRU(input_size = input_size, \n",
    "                          hidden_size = hidden, \n",
    "                          num_layers = 1,\n",
    "                          bidirectional = bd_flag, \n",
    "                          batch_first = True)\n",
    "        \n",
    "        #we want to use the output of the Hidden Layer for the next time step\n",
    "        self.HL = nn.Linear(hidden + hidden*bd_flag,10) #as size of the output is 10\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "        self.bd_flag = bd_flag\n",
    "        self.hidden  = hidden\n",
    "        \n",
    "    \n",
    "    def forward(self,x): #defines the forward pass and also the structure of the network thus helping backprop\n",
    "        \n",
    "        # x :(batch_size, #time_steps, input_size)\n",
    "        # out:(batch_size, #time_steps, output_size)\n",
    "        # h :(D*#hidden_layers, batch_size, hidden_size)\n",
    "        # D = 2 if bidirectional else 1\n",
    "        \n",
    "        \n",
    "        #initializing the hidden state to all zeros for the first input\n",
    "        \n",
    "        h_0 = torch.zeros(1 + self.bd_flag*1, x.size(0), self.hidden).requires_grad_()\n",
    "\n",
    "        # We detach as we're doing truncated BPTT and don't wanna start from the beginning for a new batch\n",
    "        \n",
    "        # Forward prop\n",
    "        out, h_n = self.gru(x, (h_0.detach()))\n",
    "\n",
    "        #we want the output at the last time step alone\n",
    "        out = self.HL(out[:,-1,:]) #obtain the output of the last hidden state\n",
    "\n",
    "        pred   = self.logsoftmax(out)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "\n",
    "class Binary_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size = 2,hidden = 128,bd_flag = False): #class constructor with params for hidden layer and input size\n",
    "        super(Binary_LSTM, self).__init__() #calls the parent constructor\n",
    "        \n",
    "        #configuring the RNN\n",
    "        self.lstm = nn.LSTM(input_size = input_size, \n",
    "                          hidden_size = hidden, \n",
    "                          num_layers = 1,\n",
    "                          bidirectional = bd_flag, \n",
    "                          batch_first = True)\n",
    "        \n",
    "        #we want to use the output of the Hidden Layer for the next time step\n",
    "        self.HL = nn.Linear(hidden + hidden*bd_flag,1) #as size of the output is 1\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.bd_flag = bd_flag\n",
    "        self.hidden  = hidden\n",
    "        \n",
    "    \n",
    "    def forward(self,x): #defines the forward pass and also the structure of the network thus helping backprop\n",
    "        \n",
    "        # x :(batch_size, #time_steps, input_size)\n",
    "        # out:(batch_size, #time_steps, output_size)\n",
    "        # h :(D*#hidden_layers, batch_size, hidden_size)\n",
    "        # c :(D*#hidden_layers, batch_size, hidden_size)\n",
    "        # D = 2 if bidirectional else 1 \n",
    "        \n",
    "        #initializing the cell and hidden state to all zeros for the first input\n",
    "        \n",
    "        h_0 = torch.zeros(1 + 1*self.bd_flag, x.size(0), self.hidden).requires_grad_()\n",
    "\n",
    "        c_0 = torch.zeros(1 + 1*self.bd_flag, x.size(0), self.hidden).requires_grad_()\n",
    "\n",
    "        # We detach as we're doing truncated BPTT and don't wanna start from the beginning for a new batch\n",
    "        \n",
    "        # Forward prop\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0.detach(), c_0.detach()))\n",
    "\n",
    "        #we want the output at every time step\n",
    "        \n",
    "\n",
    "        pred   = self.sigmoid(self.HL(out))\n",
    "    \n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, target): #custom cross entropy loss function as normal Pytorch doesn't support\n",
    "    return (1/2)*(torch.mean(-torch.sum(target * torch.log(pred))) + torch.mean(-torch.sum((1-target) * torch.log(1-pred))))\n",
    "\n",
    "\n",
    "#Defining a function to train the network: Returns the training loss for the current epoch \n",
    "def Train(model,model_flag,device,TrainDataLoader,optimizer,lossfn,train_length,reg = False,l2_reg = 0.001):\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    \n",
    "    #initializing the total training loss and total correct training predictions to 0\n",
    "    train_loss    = 0\n",
    "    train_correct = 0 #correct predictions made\n",
    "    \n",
    "    #loop over the training set\n",
    "    \n",
    "    for (data,label) in tqdm(TrainDataLoader): #(data,label): Training data for that batch\n",
    "        \n",
    "        \n",
    "        (data,label) = (data.to(device),label.to(device))  #sending the data to the device we've chosen\n",
    "        \n",
    "        #reshape data to format of the RNNs (batch_size, #time_steps, input_size)\n",
    "        \n",
    "        data = data.view(-1,28,28) #this tells the RNN there are 28 time steps it has to look at\n",
    "        \n",
    "        pred = model(data) #prediction from model\n",
    "        \n",
    "        loss = lossfn(pred,label) #our loss\n",
    "        \n",
    "        if(reg == True): #apply regularization to only input to hidden weights\n",
    "            \n",
    "            if(model_flag == 0):\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.rnn.weight_ih_l0)\n",
    "            \n",
    "            elif(model_flag == 1):\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.lstm.weight_ih_l0)\n",
    "            \n",
    "            elif(model_flag == 2):\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.gru.weight_ih_l0)\n",
    "            \n",
    " \n",
    "            loss = loss + l2_reg*l2_norm\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad() #zeroing out the gradients before backprop\n",
    "        loss.backward()       #backprop from the loss\n",
    "        optimizer.step()      #updating the weights\n",
    "        \n",
    "        #Adding this loss to  training loss and computing correct predictions\n",
    "        \n",
    "        train_loss    += loss\n",
    "        train_correct += (pred.argmax(1) == label).type(torch.float).sum().item() #our prediction with max probability is our label\n",
    "        \n",
    "        \n",
    "    #Computing training accuracy \n",
    "    \n",
    "    train_correct /= train_length #training accuracy \n",
    "    \n",
    "    return train_loss, train_correct #returning loss and accuracy \n",
    "        \n",
    "#Defining a function to test the network: Returns the test loss and prediction accuracy for the current epoch\n",
    "def Test(model,device,TestDataLoader,lossfn,test_length,reg = False,l2_reg = 0.001):\n",
    "    \n",
    "    model.eval()  #setting the model in eval/test mode\n",
    "    \n",
    "    #initializing the total test loss and total correct test predictions to 0\n",
    "    test_loss    = 0\n",
    "    test_correct = 0 #correct predictions made\n",
    "    \n",
    "    #switching off the gradient for eval\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #loop over the test set\n",
    "        \n",
    "        for (data,label) in TestDataLoader: # (data,label): Test data for that batch\n",
    "            \n",
    "            (data,label) = (data.to(device),label.to(device))  #sending the data to the device we've chosen\n",
    "            \n",
    "            #reshape data to format of the RNNs (batch_size, #time_steps, input_size)\n",
    "        \n",
    "            data = data.view(-1,28,28) #this tells the RNN there are 28 time steps it has to look at\n",
    "        \n",
    "            #perform forward pass and compute the loss\n",
    "        \n",
    "            pred = model(data) #our prediction\n",
    "            loss = lossfn(pred,label) #loss \n",
    "            \n",
    "            if(reg == True): #apply regularization to only input to hidden weights\n",
    "            \n",
    "                if(model_flag == 0):\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in model.rnn.weight_ih_l0)\n",
    "            \n",
    "                elif(model_flag == 1):\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in model.lstm.weight_ih_l0)\n",
    "            \n",
    "                elif(model_flag == 2):\n",
    "                    l2_norm = sum(p.pow(2.0).sum() for p in model.gru.weight_ih_l0)\n",
    "            \n",
    " \n",
    "                loss = loss + l2_reg*l2_norm\n",
    "            \n",
    "            #Adding this loss to  test loss and computing correct predictions\n",
    "        \n",
    "            test_loss    += loss\n",
    "            test_correct += (pred.argmax(1) == label).type(torch.float).sum().item() #our prediction with max probability is our label\n",
    "        \n",
    "        \n",
    "    #Computing prediction accuracy \n",
    "    \n",
    "    test_correct /= test_length  #prediction accuracy \n",
    "    \n",
    "    return test_loss, test_correct #returning loss and accuracy \n",
    "        \n",
    "        \n",
    "def random_pred(model,device,test_data): #predicts the label of random images\n",
    "    \n",
    "    data_ind  = [6003,416,6754,1605,5055,7965,517,5551,7070,6420]\n",
    "    \n",
    "    for ind in data_ind: #make predictions for all these indices\n",
    "        \n",
    "        (test_image,true_label) = test_data[ind]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            pred = model.forward(test_image.view(-1,28,28)).detach().cpu().numpy() #as it is a single image we directly run the forward pass\n",
    "\n",
    "            pred_class = np.argmax(pred) #predicted class\n",
    "            \n",
    "            print(f\"True label:{true_label} predicted as {pred_class}\")\n",
    "            \n",
    "\n",
    "def sequence_generator(L,batch_size = batch_size,K = 1):\n",
    "    \n",
    "    random_seq = np.random.randint(0, 9,(batch_size, L)) #generated random number sequence\n",
    "\n",
    "    x = np.zeros((batch_size, L,10)) #second dimension is 10 as we're looking at one-hot vectors\n",
    "    y = np.zeros((batch_size,10)) #output\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x[i,np.arange(L), random_seq[i]] = 1   #does everything at one go (np.arange(L) and random_seq[i]) iterate over the batch in a single step\n",
    "        y[i,random_seq[i,K]] = 1\n",
    "        \n",
    "    #converting to torch    \n",
    "    random_seq = torch.tensor(random_seq, dtype=torch.int) \n",
    "    x = torch.tensor(x, dtype=torch.int)\n",
    "    y = torch.tensor(y, dtype=torch.int)\n",
    "    \n",
    "\n",
    "    return random_seq,x.float(),y #as input x is reqd to be float \n",
    "\n",
    "def Train_sequence(model,model_flag,optimizer,lossfn,N_iter_train = N_iter_train):\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    #initializing the total training loss and total correct training predictions to 0\n",
    "    train_loss    = 0\n",
    "    train_correct = 0 #correct predictions made\n",
    "    \n",
    "    train_length = batch_size*N_iter_train\n",
    "    \n",
    "    for i in range(N_iter_train):\n",
    "        \n",
    "        L = np.random.randint(3,10) #randomizing L\n",
    "        \n",
    "        random_seq,x,y = sequence_generator(L)\n",
    "        \n",
    "        pred = model(x) #prediction using the input data\n",
    "        \n",
    "        loss = lossfn(pred,y.argmax(axis = 1))\n",
    "        \n",
    "        optimizer.zero_grad() #zeroing out the gradients before backprop\n",
    "        loss.backward()       #backprop from the loss\n",
    "        optimizer.step()      #updating the weights\n",
    "        \n",
    "        #Adding this loss to  training loss and computing correct predictions\n",
    "        train_loss    += loss\n",
    "        train_correct += (np.asarray(pred.argmax(axis = 1)-y.argmax(axis = 1))==0).sum() #as subtraction will result in 0 for correct pred\n",
    "        \n",
    "    #Computing training accuracy \n",
    "    \n",
    "    train_correct /= train_length #training accuracy \n",
    "    \n",
    "    return train_loss, train_correct #returning loss and accuracy \n",
    "\n",
    "\n",
    "def Test_sequence(model,lossfn,N_iter_test = N_iter_test):\n",
    "    \n",
    "    model.eval()  #setting the model in eval/test mode\n",
    "    \n",
    "    #initializing the total test loss and total correct test predictions to 0\n",
    "    test_loss    = 0\n",
    "    test_correct = 0 #correct predictions made\n",
    "    test_length = batch_size*N_iter_test\n",
    "    \n",
    "    #switching off the gradient for eval\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(N_iter_test):\n",
    "        \n",
    "            L = np.random.randint(3,10) #randomizing L\n",
    "        \n",
    "            random_seq,x,y = sequence_generator(L)\n",
    "        \n",
    "            pred = model(x) #prediction using the input data\n",
    "        \n",
    "            loss = lossfn(pred,y.argmax(axis = 1))\n",
    "            \n",
    "            #Adding this loss to  testing loss and computing correct predictions\n",
    "            test_loss    += loss\n",
    "            test_correct += (np.asarray(pred.argmax(axis = 1)-y.argmax(axis = 1))==0).sum() #as subtraction will result in 0 for correct pred\n",
    "        \n",
    "    #Computing prediction accuracy \n",
    "    \n",
    "    test_correct /= test_length #prediction accuracy \n",
    "    \n",
    "    return test_loss, test_correct #returning loss and accuracy \n",
    "\n",
    "def binary_sequence_generator(L,batch_size = batch_size):\n",
    "    \n",
    "    x = np.zeros((batch_size, L+1 ,2)) #we zero pad as output sequence might have dimension L+1\n",
    "    y = np.zeros((batch_size,L+1)) #output\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a = np.random.randint(0,2**(L)) #number 1\n",
    "        b = np.random.randint(0,2**L) #number 2\n",
    "        c = a+b #sum\n",
    "        \n",
    "        \n",
    "        bin_a = bin(a)[2:]\n",
    "        bin_a = list(str('0')*(L+1 - len(bin_a)) + bin_a) #sign extension\n",
    "        bin_a = np.asarray(bin_a[::-1],dtype = int) #converting to numpy array and reversing the string\n",
    "        \n",
    "        bin_b = bin(b)[2:]\n",
    "        bin_b = list(str('0')*(L+1 - len(bin_b)) + bin_b) #sign extension\n",
    "        bin_b = np.asarray(bin_b[::-1],dtype = int) #converting to numpy array and reversing the string\n",
    "\n",
    "        \n",
    "        bin_c = bin(c)[2:]\n",
    "        bin_c = list(str('0')*(L+1 - len(bin_c)) + bin_c) #sign extension\n",
    "        bin_c = np.asarray(bin_c[::-1],dtype = int) #converting to numpy array and reversing the string\n",
    "        \n",
    "        \n",
    "        \n",
    "        x[i,:,0] = bin_a\n",
    "        x[i,:,1] = bin_b\n",
    "        y[i]     = bin_c\n",
    "        \n",
    "    x = torch.tensor(x, dtype=torch.int)\n",
    "    y = torch.tensor(y, dtype=torch.int)\n",
    "\n",
    "    return x.float(),y #returning float as that is what is used in forward pass\n",
    "        \n",
    "        \n",
    "def Train_sum(model,model_flag,optimizer,lossfn,N_iter_train = N_iter_train,loss_flag = 1,L = 3): #loss_flag = 1 for MSE and 0 for CE\n",
    "    \n",
    "    model.train() #setting the model in training mode\n",
    "    #initializing the total training loss and total correct training predictions to 0\n",
    "    train_loss    = 0\n",
    "    train_correct = 0 #correct predictions made\n",
    "    \n",
    "    train_length = batch_size*N_iter_train\n",
    "    \n",
    "    for i in range(N_iter_train):\n",
    "        \n",
    "        x,y = binary_sequence_generator(L)\n",
    "        \n",
    "        pred = model(x) #prediction using the input data (explicitly make it float)\n",
    "        \n",
    "        if(loss_flag == 0): #CE Loss\n",
    "            \n",
    "            loss = cross_entropy(pred,y.view(pred.size()))\n",
    "        \n",
    "        elif(loss_flag == 1): #MSE Loss\n",
    "            \n",
    "            loss = lossfn(pred,y.view(pred.size()).float()) #converting to float for MSE loss\n",
    "        \n",
    "        optimizer.zero_grad() #zeroing out the gradients before backprop\n",
    "        loss.backward()       #backprop from the loss\n",
    "        optimizer.step()      #updating the weights\n",
    "        \n",
    "        \n",
    "        #prediction made by LSTM\n",
    "        threshold = torch.Tensor([0.5])\n",
    "        pred_y = (pred > threshold).float() * 1\n",
    "        \n",
    "        #convert to base 10 equivalent\n",
    "        pred_y = pred_y.numpy()[:,:,0]\n",
    "        pred_y = pred_y.dot(2**np.arange(pred_y.shape[1]))\n",
    "\n",
    "        y_10 = y.numpy()\n",
    "        y_10 = y_10.dot(2**np.arange(y_10.shape[1]))\n",
    "        \n",
    "        #Adding this loss to  training loss and computing correct predictions\n",
    "        train_loss    += loss\n",
    "        train_correct += np.sum(pred_y == y_10) #as subtraction will result in 0 for correct pred, bitwise accuracy\n",
    "       \n",
    "    #Computing training accuracy \n",
    "    \n",
    "    train_correct /= train_length #training accuracy \n",
    "    \n",
    "    return train_loss, train_correct #returning loss and accuracy \n",
    "                          \n",
    "\n",
    "    \n",
    "        \n",
    "def Test_sum(model,lossfn,N_iter_test = 100,loss_flag = 1,L = 9): #as we're supposed to test on 100 samples\n",
    "    \n",
    "    model.eval()  #setting the model in eval/test mode\n",
    "    \n",
    "    #initializing the total test loss and total correct test predictions to 0\n",
    "    test_loss    = 0\n",
    "    test_correct = 0 #correct predictions made\n",
    "    test_length = batch_size*N_iter_test\n",
    "    \n",
    "    #switching off the gradient for eval\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(N_iter_test):\n",
    "        \n",
    "            x,y = binary_sequence_generator(L)\n",
    "        \n",
    "        \n",
    "            pred = model(x.float()) #prediction using the input data\n",
    "        \n",
    "            if(loss_flag == 0): #CE Loss\n",
    "            \n",
    "                loss = cross_entropy(pred,y.view(pred.size()))\n",
    "        \n",
    "            elif(loss_flag == 1): #MSE Loss\n",
    "            \n",
    "                loss = lossfn(pred,y.view(pred.size()).float()) #converting to float for MSE loss\n",
    "                \n",
    "                          \n",
    "            #prediction made by LSTM\n",
    "            threshold = torch.Tensor([0.5])\n",
    "            pred_y = (pred > threshold).float() * 1\n",
    "            \n",
    "            #convert to base 10 equivalent\n",
    "            pred_y = pred_y.numpy()[:,:,0]\n",
    "            pred_y = pred_y.dot(2**np.arange(pred_y.shape[1]))\n",
    "\n",
    "            y_10 = y.numpy()\n",
    "            y_10 = y_10.dot(2**np.arange(y_10.shape[1]))\n",
    "    \n",
    "            #Adding this loss to  testing loss and computing correct predictions\n",
    "            test_loss    += loss\n",
    "            test_correct += np.sum(pred_y==y_10) #as subtraction will result in 0 for correct pred\n",
    "        \n",
    "    #Computing prediction accuracy \n",
    "    \n",
    "    test_correct /= test_length #prediction accuracy \n",
    "    \n",
    "    return test_loss, test_correct #returning loss and accuracy     \n",
    "                          \n",
    "\n",
    "def Check_sequence(model,N_iter_check = N_iter_check):\n",
    "    \n",
    "    model.eval()  #setting the model in eval/test mode\n",
    "    \n",
    "    #switching off the gradient for eval\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(N_iter_check):\n",
    "        \n",
    "            L = np.random.randint(3,10) #randomizing L\n",
    "        \n",
    "            random_seq,x,y = sequence_generator(L,batch_size = 1)\n",
    "        \n",
    "            pred = model(x) #prediction using the input data\n",
    "            \n",
    "            print(f'Generated Sequence:{random_seq}')\n",
    "            print(f'Predicted Output:{pred.argmax(axis = 1)}')\n",
    "            \n",
    "\n",
    "def Check_binary_sequence(model,model_name,lossfn,N_check = 100,loss_flag = 1):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracies = []\n",
    "    \n",
    "    for L in range(1,21): #iterating through L in the required range\n",
    "    \n",
    "        #initializing the total test loss and total correct test predictions to 0\n",
    "        test_loss    = 0\n",
    "        test_correct = 0 #correct predictions made\n",
    "        test_length = 100\n",
    "    \n",
    "        #switching off the gradient for eval\n",
    "        with torch.no_grad():\n",
    "            \n",
    "        \n",
    "            x,y = binary_sequence_generator(L,batch_size = N_check)\n",
    "        \n",
    "        \n",
    "            pred = model(x.float()) #prediction using the input data\n",
    "        \n",
    "            if(loss_flag == 0): #CE Loss\n",
    "            \n",
    "                loss = cross_entropy(pred,y.view(pred.size()))\n",
    "        \n",
    "            elif(loss_flag == 1): #MSE Loss\n",
    "            \n",
    "                loss = lossfn(pred,y.view(pred.size()).float()) #converting to float for MSE loss\n",
    "                \n",
    "                          \n",
    "            #prediction made by LSTM\n",
    "            threshold = torch.Tensor([0.5])\n",
    "            pred_y = (pred > threshold).float() * 1\n",
    "            \n",
    "            #convert to base 10 equivalent\n",
    "            pred_y = pred_y.numpy()[:,:,0]\n",
    "            pred_y = pred_y.dot(2**np.arange(pred_y.shape[1]))\n",
    "\n",
    "            y_10 = y.numpy()\n",
    "            y_10 = y_10.dot(2**np.arange(y_10.shape[1]))\n",
    "    \n",
    "            #Adding this loss to  testing loss and computing correct predictions\n",
    "            test_loss    += loss\n",
    "            test_correct += np.sum(pred_y==y_10) #as subtraction will result in 0 for correct pred\n",
    "        \n",
    "            #Computing prediction accuracy \n",
    "    \n",
    "            test_correct /= test_length #prediction accuracy\n",
    "            \n",
    "            test_accuracies.append(test_correct)\n",
    "            \n",
    "    \n",
    "    #plotting test accuracies vs L\n",
    "    \n",
    "    plt.bar(np.arange(1,21),test_accuracies)\n",
    "    plt.xlabel('<-- L -->')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0,1) #as accuracy is between 0 and 1\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Prediction Accuracy vs Length')\n",
    "    plt.savefig(download_dir+'\\\\'+model_name+str(loss_flag)+'_L_accuracy.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Prediction Accuracies : {test_accuracies}')\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_RNN(model_name,load_model = load_model,model_flag = 0,bd_flag = False,reg = False,hidden_layer = 128,question_flag = 1,N_epochs = N_epochs,learning_rate = learning_rate,batch_size = batch_size,loss_flag = 1,L=3):\n",
    "    \n",
    "    startTime = time.time()\n",
    "    #applied transform first converts the data into a tensor then normalizes it.\n",
    "    #0.1307 is the mean of the MNIST data set and 0.3081 is the standard deviation\n",
    "    \n",
    "    app_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    #organize the training and test data\n",
    "    \n",
    "    train_data    = MNIST(mnist_dir, train = True, download = download_flag, transform = app_transform) #getting training data\n",
    "    test_data     = MNIST(mnist_dir, train = False, transform = app_transform)\n",
    "    \n",
    "    #initialize the dataloaders\n",
    "    TrainDataLoader = DataLoader(train_data, batch_size = batch_size, shuffle = True ) \n",
    "    TestDataLoader  = DataLoader(test_data, batch_size = batch_size) \n",
    "    \n",
    "    train_length  = len(TrainDataLoader.dataset) #no of training examples\n",
    "    test_length   = len(TestDataLoader.dataset)  #no of testing cases\n",
    "    \n",
    "    #initialize the loss function\n",
    "    \n",
    "    lossfn    = nn.NLLLoss() #like a cross entropy loss function when we couple this with softmax\n",
    "    \n",
    "    #initialize the model\n",
    "    if(model_flag == 0): #RNN\n",
    "        \n",
    "        model = Vanilla_RNN(hidden = hidden_layer,bd_flag = bd_flag).to(device)\n",
    "        \n",
    "    \n",
    "    elif(model_flag == 1): #LSTM\n",
    "        \n",
    "        model = Vanilla_LSTM(hidden = hidden_layer,bd_flag = bd_flag).to(device)\n",
    "        \n",
    "    elif(model_flag == 2): #GRU\n",
    "        \n",
    "        model = Vanilla_GRU(hidden = hidden_layer,bd_flag = bd_flag).to(device)\n",
    "    \n",
    "    elif(model_flag == 3): #binary_LSTM\n",
    "        \n",
    "        model = Binary_LSTM(hidden = hidden_layer,bd_flag = bd_flag).to(device)\n",
    "        \n",
    "        #change loss function\n",
    "        lossfn    = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    #initialize the optimizer\n",
    "    optimizer = Adam(model.parameters(),lr = learning_rate) #using Adam for GD as its the fastest and state of the art\n",
    "    \n",
    "\n",
    "    if (load_model): #if the load_model flag is true, load the pre-trained model\n",
    "        \n",
    "        print('Loading Model')\n",
    "        model.load_state_dict(torch.load(model_path + model_name), strict=False)\n",
    "        print(model)\n",
    "        \n",
    "        if(question_flag == 1):\n",
    "            random_pred(model,device,test_data)\n",
    "        \n",
    "        if(question_flag == 2):\n",
    "            Check_sequence(model)\n",
    "            \n",
    "        if(question_flag == 3):\n",
    "            Check_binary_sequence(model,model_name,lossfn,loss_flag = loss_flag)\n",
    "               \n",
    "        \n",
    "    else: #train and test the RNN network if load_model flag is false\n",
    "        \n",
    "        if(question_flag == 1):\n",
    "        \n",
    "        \n",
    "            #initialising the lists\n",
    "\n",
    "            train_losses   = []\n",
    "            test_losses    = []\n",
    "            train_accuracy = []\n",
    "            test_accuracy  = []\n",
    "\n",
    "            for epoch in range(1, N_epochs+1):\n",
    "                print(\"Epoch \",epoch,\" has just begun!\")\n",
    "                print('****************** ', epoch/N_epochs,\" % ******************\") #creates a status bar instead of using tqdm\n",
    "\n",
    "                #train the model\n",
    "                loss,accuracy = Train(model,model_flag,device,TrainDataLoader,optimizer,lossfn,train_length,reg = False,l2_reg = 0.001)\n",
    "                train_losses.append(loss)\n",
    "                train_accuracy.append(accuracy)\n",
    "                print('Train loss for Epoch ',epoch,': ',loss)\n",
    "                print('Train accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "                #test the model\n",
    "                loss,accuracy = Test(model,device,TestDataLoader,lossfn,test_length,reg = False,l2_reg = 0.001)\n",
    "                test_losses.append(loss)\n",
    "                test_accuracy.append(accuracy)\n",
    "                print('Test loss for Epoch ',epoch,': ',loss)\n",
    "                print('Test accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "            endTime = time.time()  \n",
    "            print(\"Time taken to train and test model: \",endTime-startTime)\n",
    "            print(\"Average prediction accuracy across epochs: \",np.mean(test_accuracy))\n",
    "            print(\"Average training accuracy across epochs: \",np.mean(train_accuracy))\n",
    "\n",
    "            #Plotting the Loss and the accuracy curves\n",
    "\n",
    "            plt.plot(np.asfarray(train_losses),'o-',label = 'Train Loss') #converting to float array\n",
    "            plt.plot(np.asfarray(test_losses),'o-',label = 'Validation Loss') \n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Normalized Loss')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Validation error with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_loss.png')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(np.asfarray(train_accuracy),'o-',label = 'Training Accuracy')\n",
    "            plt.plot(np.asfarray(test_accuracy),'o-',label = 'Testing Accuracy')\n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.ylim(0,1) #as accuracy is between 0 and 1\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Prediction Accuracy with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_accuracy.png')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            #generating a classification report\n",
    "\n",
    "            #final predictions\n",
    "\n",
    "            preds = [] #list to store our final predictions\n",
    "\n",
    "            #switching off the gradient for eval\n",
    "            with torch.no_grad():\n",
    "\n",
    "                #loop over the test set\n",
    "\n",
    "                for (data,label) in TestDataLoader: # (data,label): Test data for that batch\n",
    "\n",
    "                    (data,label) = (data.to(device),label.to(device))  #sending the data to the device we've chosen\n",
    "\n",
    "                    #reshape data to format of the RNNs (batch_size, #time_steps, input_size)\n",
    "\n",
    "                    data = data.view(-1,28,28) #this tells the RNN there are 28 time steps it has to look at\n",
    "\n",
    "                    #perform forward pass and compute the loss\n",
    "\n",
    "                    pred = model(data) #our prediction\n",
    "\n",
    "                    #add our predictions to the list\n",
    "                    preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "\n",
    "            print(classification_report(test_data.targets.cpu().numpy(), np.asfarray(preds), target_names=test_data.classes))\n",
    "\n",
    "\n",
    "            #plotting the confusion matrix\n",
    "            confusion_mat = confusion_matrix(test_data.targets.cpu().numpy(), np.asfarray(preds))\n",
    "\n",
    "            plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "            plt.colorbar()\n",
    "            classNames = ['0','1','2','3','4','5','6','7','8','9']\n",
    "            plt.title('Test Data: Confusion Matrix')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            tick_marks = np.arange(len(classNames))\n",
    "            plt.xticks(tick_marks, classNames)\n",
    "            plt.yticks(tick_marks, classNames)\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_confusion.png')\n",
    "            plt.show()\n",
    "\n",
    "            # Save the model we just trained\n",
    "            torch.save(model.state_dict(), model_path+model_name)\n",
    "            print(model)\n",
    "        \n",
    "        \n",
    "        elif(question_flag == 2):\n",
    "            \n",
    "            #initialising the lists\n",
    "\n",
    "            train_losses   = []\n",
    "            test_losses    = []\n",
    "            train_accuracy = []\n",
    "            test_accuracy  = []\n",
    "\n",
    "            for epoch in range(1, N_epochs+1):\n",
    "                print(\"Epoch \",epoch,\" has just begun!\")\n",
    "                print('****************** ', epoch/N_epochs,\" % ******************\") #creates a status bar instead of using tqdm\n",
    "\n",
    "                #train the model\n",
    "                loss,accuracy = Train_sequence(model,model_flag,optimizer,lossfn,N_iter_train = N_iter_train)\n",
    "                train_losses.append(loss)\n",
    "                train_accuracy.append(accuracy)\n",
    "                print('Train loss for Epoch ',epoch,': ',loss)\n",
    "                print('Train accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "                #test the model\n",
    "                loss,accuracy = Test_sequence(model,lossfn,N_iter_test = N_iter_test)\n",
    "                test_losses.append(loss)\n",
    "                test_accuracy.append(accuracy)\n",
    "                print('Test loss for Epoch ',epoch,': ',loss)\n",
    "                print('Test accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "            endTime = time.time()  \n",
    "            print(\"Time taken to train and test model: \",endTime-startTime)\n",
    "            print(\"Average prediction accuracy across epochs: \",np.mean(test_accuracy))\n",
    "            print(\"Average training accuracy across epochs: \",np.mean(train_accuracy))\n",
    "\n",
    "            #Plotting the Loss and the accuracy curves\n",
    "\n",
    "            plt.plot(np.asfarray(train_losses),'o-',label = 'Train Loss') #converting to float array\n",
    "            plt.plot(np.asfarray(test_losses),'o-',label = 'Validation Loss') \n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Normalized Loss')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Validation error with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_loss.png')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(np.asfarray(train_accuracy),'o-',label = 'Training Accuracy')\n",
    "            plt.plot(np.asfarray(test_accuracy),'o-',label = 'Testing Accuracy')\n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.ylim(0,1) #as accuracy is between 0 and 1\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Prediction Accuracy with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_accuracy.png')\n",
    "            plt.show()\n",
    "            \n",
    "            # Save the model we just trained\n",
    "            torch.save(model.state_dict(), model_path+model_name)\n",
    "            \n",
    "        elif(question_flag == 3):    \n",
    "            \n",
    "            #initialising the lists\n",
    "\n",
    "            train_losses   = []\n",
    "            test_losses    = []\n",
    "            train_accuracy = []\n",
    "            test_accuracy  = []\n",
    "\n",
    "            for epoch in range(1, N_epochs+1):\n",
    "                print(\"Epoch \",epoch,\" has just begun!\")\n",
    "                print('****************** ', epoch/N_epochs,\" % ******************\") #creates a status bar instead of using tqdm\n",
    "\n",
    "                #train the model\n",
    "                loss,accuracy = Train_sum(model,model_flag,optimizer,lossfn,N_iter_train = N_iter_train,loss_flag = loss_flag, L = L)\n",
    "                train_losses.append(loss)\n",
    "                train_accuracy.append(accuracy)\n",
    "                print('Train loss for Epoch ',epoch,': ',loss)\n",
    "                print('Train accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "                #test the model\n",
    "                loss,accuracy = Test_sum(model,lossfn,N_iter_test = N_iter_test,loss_flag = loss_flag, L = L)\n",
    "                test_losses.append(loss)\n",
    "                test_accuracy.append(accuracy)\n",
    "                print('Test loss for Epoch ',epoch,': ',loss)\n",
    "                print('Test accuracy for Epoch ',epoch, ': ',accuracy)\n",
    "\n",
    "            endTime = time.time()  \n",
    "            print(\"Time taken to train and test model: \",endTime-startTime)\n",
    "            print(\"Average prediction accuracy across epochs: \",np.mean(test_accuracy))\n",
    "            print(\"Average training accuracy across epochs: \",np.mean(train_accuracy))\n",
    "\n",
    "            #Plotting the Loss and the accuracy curves\n",
    "\n",
    "            plt.plot(np.asfarray(train_losses),'o-',label = 'Train Loss') #converting to float array\n",
    "            plt.plot(np.asfarray(test_losses),'o-',label = 'Validation Loss') \n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Normalized Loss')\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Validation error with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+'_loss.png')\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(np.asfarray(train_accuracy),'o-',label = 'Training Accuracy')\n",
    "            plt.plot(np.asfarray(test_accuracy),'o-',label = 'Testing Accuracy')\n",
    "            plt.xlabel('<-- Epochs -->')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.ylim(0,1) #as accuracy is between 0 and 1\n",
    "            plt.grid()\n",
    "            plt.legend()\n",
    "            plt.title('Progress of Training and Prediction Accuracy with every Epoch')\n",
    "            plt.savefig(download_dir+'\\\\'+model_name+str(loss_flag)+'_accuracy.png')\n",
    "            plt.show()\n",
    "            \n",
    "            # Save the model we just trained\n",
    "            torch.save(model.state_dict(), model_path+model_name)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Assignment():\n",
    "    \n",
    "    answer = str(input('Do you want to check the results of question 1? (y/n)'))\n",
    "    \n",
    "    if(answer == 'y'):\n",
    "        \n",
    "        while(answer == 'y'):\n",
    "        \n",
    "            answer = str(input('Do you want to train an RNN model?'))\n",
    "        \n",
    "            if(answer == 'y'):\n",
    "            \n",
    "                print('Given below are the choices')\n",
    "            \n",
    "                model_flag = int(input('Please enter 0 for RNN, 1 for LSTM and 2 for GRU'))\n",
    "                hidden_layer = int(input('Please enter the size of the Hidden Layer (128 is default)'))\n",
    "                bd_flag      = str(input('Do you want a bidirectional model? (y/n)'))\n",
    "                reg_flag     = str(input('Do you want regularization on input to hidden weights?'))\n",
    "                \n",
    "            \n",
    "                if(bd_flag == 'y'):\n",
    "                    bd_flag = True\n",
    "                else:\n",
    "                    bd_flag = False\n",
    "                \n",
    "                if(reg_flag == 'y'):\n",
    "                    reg_flag = True\n",
    "                else:\n",
    "                    reg_flag = False\n",
    "                \n",
    "                model_name = 'RNN'*(model_flag == 0) + 'LSTM'*(model_flag == 1) + 'GRU'*(model_flag == 2) +'_'+str(hidden_layer)+'_bd'*(bd_flag)+'_reg'*(reg_flag)+'.mdl'                \n",
    "                Run_RNN(model_name,load_model = False,model_flag = model_flag,hidden_layer = hidden_layer,bd_flag = bd_flag)\n",
    "            \n",
    "                answer = str(input('Do you want to check test predictions?'))\n",
    "                if(answer == 'y'):\n",
    "                    Run_RNN(model_name,load_model = True,model_flag = model_flag,hidden_layer = hidden_layer,bd_flag = bd_flag)\n",
    "                \n",
    "                \n",
    "            answer = str(input('Do you want to repeat this experiment again?'))        \n",
    "                \n",
    "    answer = str(input('Do you want to check the results of question 2? (y/n)'))\n",
    "    \n",
    "    if(answer == 'y'):\n",
    "        \n",
    "        answer = str(input('Do you want to train an RNN model?'))\n",
    "        \n",
    "        if(answer == 'y'):\n",
    "            \n",
    "            print('Given below are the choices')\n",
    "            \n",
    "            model_flag = int(input('Please enter 0 for RNN, 1 for LSTM and 2 for GRU'))\n",
    "            hidden_layers = [2,5,10] \n",
    "            \n",
    "            for hidden_layer in hidden_layers:\n",
    "                answer = str(input(f'Do you want to train model with hidden layer = {hidden_layer}?'))\n",
    "                if(answer == 'y'):\n",
    "                    model_name = 'RNN'*(model_flag == 0) + 'LSTM'*(model_flag == 1) + 'GRU'*(model_flag == 2) +'_'+str(hidden_layer)+'_q2'+'.mdl'                \n",
    "                    Run_RNN(model_name,load_model = False,model_flag = model_flag,hidden_layer = hidden_layer,question_flag = 2)\n",
    "                \n",
    "                    answer = str(input('Do you want to check the model output?'))\n",
    "                    if(answer == 'y'):\n",
    "                        Run_RNN(model_name,load_model = True,model_flag = model_flag,hidden_layer = hidden_layer,question_flag = 2)\n",
    "                    \n",
    "                \n",
    "    answer = str(input('Do you want to check the results of question 3? (y/n)'))\n",
    "    \n",
    "    if(answer == 'y'):\n",
    "        \n",
    "        answer = str(input('Do you want to train an LSTM model?'))\n",
    "        if(answer == 'y'):\n",
    "            hidden = int(input('Enter size of hidden layer'))\n",
    "            choice = int(input('Please enter 1 for MSE Loss and 0 for cross entropy loss'))\n",
    "            if(choice == 0):\n",
    "                loss_flag = 0\n",
    "            elif(choice == 1):\n",
    "                loss_flag = 1\n",
    "            L = int(input('Enter the sequence size you want to train the model on'))\n",
    "            model_name =  'LSTM' +'_'+str(hidden)+str(L)+'_q3'+'.mdl'\n",
    "                \n",
    "            Run_RNN(model_name,load_model = False,model_flag = 3,hidden_layer = hidden,question_flag = 3,loss_flag = loss_flag,L = L)\n",
    "            \n",
    "            answer = str(input('Do you want to check predictions across different lengths?'))\n",
    "            if(answer == 'y'):\n",
    "                Run_RNN(model_name,load_model = True,model_flag = 3,hidden_layer = hidden,question_flag = 3,loss_flag = loss_flag,L = L)\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell for the assignment\n",
    "Run_Assignment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
