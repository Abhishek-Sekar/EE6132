{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import wget # a python library to download files from a given url\n",
    "from mnist import MNIST #a python package which has the mnis\n",
    "import os #library corresponding to os based operations\n",
    "from PIL import Image as im #used to convert the given image array to a b/w image\n",
    "import numpy as np \n",
    "import random\n",
    "import statistics\n",
    "from tqdm import tqdm #for timing\n",
    "import matplotlib.pyplot as plt #for plots\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to download the train and test datasets from the given MNIST url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to download the mnist dataset if it hasn't been downloaded already\n",
    "\n",
    "print(\"Do you want to download the mnist dataset ?(y/n)\")\n",
    "choice = str(input())\n",
    "if(choice.lower() == 'y'): #if choice is yes, proceed with the downloads   \n",
    "    master_dir = os.getcwd() #this is the directory we're working in\n",
    "    mnist_dir = \"mnist\" #dir to store the downloaded mnist dataset\n",
    "    if(os.path.isdir(os.path.join(master_dir,mnist_dir))=='True'): #if the directory doesn't exist\n",
    "            os.mkdir(os.path.join(master_dir,mnist_dir))#make a directory called mnist in the master_dir\n",
    "\n",
    "    #defining list datasets which has strings pertaining to the necessary extensions on the mnist database which gives us the files\n",
    "\n",
    "    datasets = [\"train-images-idx3-ubyte.gz\",\"train-labels-idx1-ubyte.gz\",\"t10k-images-idx3-ubyte.gz\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "    mnist_url = \"http://yann.lecun.com/exdb/mnist/\" #url containing the dataset\n",
    "\n",
    "    for data in datasets:\n",
    "        wget.download(os.path.join(mnist_url,data),out=mnist_dir)#downloads all the files to the dir called mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to unzip and format the downloaded mnist dataset\n",
    "master_dir = os.getcwd() #this is the directory we're working in\n",
    "mnist_dir = \"mnist\" #dir to store the downloaded mnist dataset\n",
    "mnist_path = os.path.join(master_dir,mnist_dir) #change name to path where mnist is downloaded to\n",
    "\n",
    "#creating an mnist object\n",
    "\n",
    "mnist_obj = MNIST(mnist_path)\n",
    "mnist_obj.gz = True #as I downloaded the .gz zip files\n",
    "train_images,train_labels = mnist_obj.load_training() #loads the training dataset\n",
    "test_images,test_labels   = mnist_obj.load_testing() #loads the testing dataset\n",
    "\n",
    "print(\"Done extracting the data\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the resulting labels into a one_hot_vector\n",
    "N_classes = 10 #number of classes is 10 for mnist\n",
    "one_hot_train = []\n",
    "one_hot_test = []\n",
    "\n",
    "for i,label in enumerate(train_labels):\n",
    "    dummy = np.zeros(N_classes)\n",
    "    dummy[label] = 1    #wherever the label is, make that index 1\n",
    "    one_hot_train.append(dummy)\n",
    "    \n",
    "for i,label in enumerate(test_labels):\n",
    "    dummy = np.zeros(N_classes)\n",
    "    dummy[label] = 1    #wherever the label is, make that index 1\n",
    "    one_hot_test.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.asarray(train_images)\n",
    "np.transpose(train_images).shape\n",
    "np.asarray(one_hot_train).shape\n",
    "print(np.transpose(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images are of type list each image is a 784 long list\n",
    "### Labels are of type int , which we convert to one hot of length 10\n",
    "### About train: 60,000 images \n",
    "### About test:10,000 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the image just for clarity\n",
    "image_sample = np.asarray(train_images[0],dtype = np.uint8).reshape(28,28)\n",
    "\n",
    "#creating image object\n",
    "image_obj = im.fromarray(image_sample) #converts the array into a b/w image 28x28\n",
    "\n",
    "#saving the image\n",
    "image_obj.save('sample_digit.png')\n",
    "print('Sample image has been saved')\n",
    "image_obj.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifications to training_data and testing_data\n",
    "\n",
    "train_data = np.transpose(train_images)/255\n",
    "one_hot_train = np.transpose(np.asarray(one_hot_train))\n",
    "\n",
    "test_data = np.transpose(test_images)/255\n",
    "one_hot_test = np.transpose(np.asarray(one_hot_test))\n",
    "\n",
    "print(test_data.shape,one_hot_test.shape)\n",
    "print(train_data.shape,one_hot_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.transpose(test_images)\n",
    "one_hot_test = np.transpose(np.asarray(one_hot_test))\n",
    "\n",
    "print(test_data.shape,one_hot_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library of all the activation functions\n",
    "#input should be an integer array/matrix or an integer\n",
    "\n",
    "def sigmoid(x):  #input is x and output is the sigmoid of x 1/(1+exp(-x)) \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x): #input is x and output is ReLU(x),max{x,0} \n",
    "    #val = 1e-9*np.ones_like(x) #regularization\n",
    "    return x*(x > 0) # cool way to compute ReLU for vectors and scalars alike \n",
    "\n",
    "def tanh(x): #input is x and output is tanh(x) = (exp(x)-exp(-x))/(exp(x)+exp(-x))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def linear(x): #for closure input is x, output is x\n",
    "    return x\n",
    "\n",
    "def softmax(x): #input is a matrix x, output is the softmax of each element of the matrix,column-wise, ie softmax(xi) = exp(xi)/sum(exp(xj) over all j)\n",
    "    #print(max(x))\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/exp_x.sum(axis = 0) #summing along axis = 0,ie the columns of x\n",
    "\n",
    "#create a dictionary hosting the function names\n",
    "\n",
    "activation_library = {\n",
    "    1:sigmoid,\n",
    "    2:ReLU,\n",
    "    3:tanh,\n",
    "    4:linear,\n",
    "    5:softmax\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library of all the derivatives of the activation functions\n",
    "#input should be an integer array or an integer\n",
    "\n",
    "def sigmoid_derivative(x): # computes the derivative of the activation function sigmoid for input x\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def ReLU_derivative(x): #computes the derivative of the activation function ReLU for input x\n",
    "    #val = 1e-9*np.ones_like(x) #regularization\n",
    "    return 1*(x > 0)\n",
    "\n",
    "def tanh_derivative(x): #computes the derivative of the activation function tanh for input x\n",
    "    return 1- tanh(x)**2\n",
    "\n",
    "def linear_derivative(x): #computes the derivative of the linear activation function for input x\n",
    "    return 1\n",
    "\n",
    "def softmax_derivative_for_CE(p_true,q_pred): #computes the derivative for softmax activation at the final layer of the MLP\n",
    "    return q_pred - p_true\n",
    "\n",
    "    \n",
    "\n",
    "activation_derivative_library = {\n",
    "    1: sigmoid_derivative,\n",
    "    2: ReLU_derivative,\n",
    "    3: tanh_derivative,\n",
    "    4: linear_derivative,\n",
    "    5: softmax_derivative_for_CE\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rates\n",
    "def constant_learning_rate(learning_rate,i=0,thresh=9):\n",
    "    return learning_rate\n",
    "\n",
    "def exp_learning_rate(learning_rate,i=0,thresh=4): \n",
    "    if(i > thresh):\n",
    "        return learning_rate*np.exp(-i)\n",
    "    else:\n",
    "        return learning_rate\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_library = {\n",
    "    1:constant_learning_rate,\n",
    "    2:exp_learning_rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "#perform one iteration of gradient descent\n",
    "\n",
    "def basic_grad_descent(learning_rate,theta,minibatch_size,theta_grad,t,r_t=0,s_t=0,delta = 10e-10,pho_1=0.9,pho_2=0.85,bias = True,reg1 = False,reg1_lambda = 1e-3):\n",
    "    if(bias == True):\n",
    "        theta_new = theta - learning_rate*theta_grad.sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        #print(theta.shape,theta_grad.shape,theta_new.shape)\n",
    "    else:\n",
    "        theta_new = theta - learning_rate*theta_grad/minibatch_size - np.sign(theta)*reg1*reg1_lambda/minibatch_size\n",
    "    \n",
    "    return theta_new,r_t,s_t\n",
    "\n",
    "def momentum_gd(learning_rate,theta,minibatch_size,theta_grad,t,r_t = 0,s_t=0,delta = 10e-10,pho_1 = 0.9,pho_2=0.85,bias = True,reg1 = False,reg1_lambda = 1e-3): #can be converted to nesterov gd by taking derivative at theta_t - gamma*Vt-1\n",
    "    if(bias == True):\n",
    "        r_upd = pho_1*r_t + learning_rate*theta_grad.sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "    else:\n",
    "        r_upd = pho_1*r_t + learning_rate*theta_grad/minibatch_size - np.sign(theta)*reg1*reg1_lambda/minibatch_size\n",
    "        \n",
    "    theta_new = theta - r_upd\n",
    "    return theta_new,r_upd,s_t\n",
    "\n",
    "def Adagrad(learning_rate,theta,minibatch_size,theta_grad,t,r_t = 0,s_t=0,delta = 10e-10,pho_1=0.9,pho_2=0.85,bias = True,reg1 = False,reg1_lambda = 1e-3):\n",
    "    if(bias == True):\n",
    "        r_upd = r_t + np.square(theta_grad).sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        theta_new = theta - learning_rate*np.reciprocal((delta + np.sqrt(r_upd)))*theta_grad.sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        \n",
    "    else:\n",
    "        r_upd = r_t + np.square(theta_grad)/minibatch_size + reg1*(reg1_lambda/minibatch_size)**2\n",
    "        theta_new = theta - learning_rate*np.reciprocal((delta + np.sqrt(r_upd)))*theta_grad/minibatch_size - np.sign(theta)*reg1*reg1_lambda/minibatch_size\n",
    "        \n",
    "    return theta_new,r_upd,s_t\n",
    "\n",
    "def RMS_prop(learning_rate,theta,minibatch_size,theta_grad,t,r_t=0,s_t=0,delta = 10e-10,pho_1 = 0.9,pho_2=0.85,bias = True,reg1 = False,reg1_lambda = 1e-3):\n",
    "    if(bias == True):\n",
    "        r_upd = pho_1*r_t + (1-pho_1)*np.square(theta_grad).sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        theta_new = theta - learning_rate*np.reciprocal((delta + np.sqrt(r_upd)))*theta_grad.sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        \n",
    "    else:\n",
    "        r_upd = pho_1*r_t + (1-pho_1)*np.square(theta_grad)/minibatch_size + reg1*(reg1_lambda/minibatch_size)**2\n",
    "        theta_new = theta - learning_rate*np.reciprocal((delta + np.sqrt(r_upd)))*theta_grad/minibatch_size - np.sign(theta)*reg1*reg1_lambda/minibatch_size\n",
    "        \n",
    "    return theta_new,r_upd,s_t\n",
    "\n",
    "def ADAM(learning_rate,theta,minibatch_size,theta_grad,t,r_t = 0,s_t = 0,delta = 10e-10,pho_1 = 0.9,pho_2 = 0.85,bias = True):\n",
    "    if(bias == True):\n",
    "        s_upd = pho_1*s_t + (1-pho_1)*np.square(theta_grad).sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        r_upd = pho_2*r_t + (1-pho_2)*np.square(theta_grad).sum(axis = 1).reshape(theta.shape)/minibatch_size\n",
    "        \n",
    "    else:\n",
    "        s_upd = pho_1*s_t + (1-pho_1)*np.square(theta_grad)/minibatch_size\n",
    "        r_upd = pho_2*r_t + (1-pho_2)*np.square(theta_grad)/minibatch_size\n",
    "        \n",
    "    if(t == 0):\n",
    "        s_val = 0\n",
    "        r_val = 0\n",
    "    else:\n",
    "        s_val = s_upd/(1-pho_1**t)\n",
    "        r_val = r_upd/(1-pho_2**t)\n",
    "    \n",
    "    theta_new = theta - learning_rate*np.multiply(1/(delta + np.sqrt(r_val)),s_val)\n",
    "\n",
    "    return theta_new,r_upd,s_upd\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "grad_descent_library = {\n",
    "    1:basic_grad_descent,\n",
    "    2:momentum_gd,\n",
    "    3:Adagrad,\n",
    "    4:RMS_prop,\n",
    "    5:ADAM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "\n",
    "def cross_entropy_loss(p,q): #p,q are vectors, p is the true label, q is our prediction, function returns cross entropy loss: -sum(pi*log(qi))\n",
    "        epsilon = 1e-11 #epsilon regularization should qi be very small\n",
    "        return -np.mean((p*np.log(q+epsilon)).sum(0))\n",
    "    \n",
    "def l2_loss(p,q,weights,batch_size,lambda_reg = 8e-4 ):\n",
    "    reg_loss = 0\n",
    "    for i in weights.keys():\n",
    "        reg_loss += np.sum(np.square(weights[i]))\n",
    "    \n",
    "    return cross_entropy_loss(p,q) + (lambda_reg/(2*batch_size))*reg_loss  \n",
    " \n",
    "def l1_loss(p,q,weights,batch_size,lambda_reg = 1e-1):\n",
    "    reg_loss = 0\n",
    "    for i in weights.keys():\n",
    "        reg_loss += np.sum(np.abs(weights[i]))\n",
    "    \n",
    "    return cross_entropy_loss(p,q) + (lambda_reg/batch_size)*reg_loss\n",
    "\n",
    "reg_loss = {\n",
    "    1:l1_loss,\n",
    "    2:l2_loss\n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score normalization \n",
    "def basic_stats(y_pred,y_true):\n",
    "    total_labels = len(y_true[0])\n",
    "    correct_pred = ((np.argmax(y_pred,axis = 0) == np.argmax(y_true,axis = 0))*np.ones(total_labels)).sum() #sum gives no of correct pred\n",
    "    accuracy     = correct_pred/total_labels\n",
    "    true_labels  = np.asarray(list(enumerate(np.argmax(y_true,axis = 0))))\n",
    "    y_vals       = y_pred[true_labels[:,1],true_labels[:,0]]\n",
    "    mean_error   = np.mean(1-y_vals)\n",
    "    stdev_error  = statistics.stdev(1-y_vals)\n",
    "    return accuracy,mean_error,stdev_error\n",
    "\n",
    "def more_stats(y_pred,y_true):\n",
    "    confusion_mat = np.dot(y_true,y_pred.T)\n",
    "    precision        = np.diag(confusion_mat)/np.sum(confusion_mat,axis=0)\n",
    "    recall           = np.diag(confusion_mat)/np.sum(confusion_mat,axis=1)\n",
    "    f1_score         = 2*precision*recall/(precision+recall)\n",
    "    return confusion_mat,precision,recall,f1_score\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a class MLP and also the accompanying functions required to operate the MLP\n",
    "class MLP: #class named multi layer perceptron\n",
    "    \n",
    "    def __init__(self,architecture,activations,train_images,train_labels,test_images,test_labels,plot= True,plot_mode = 0,threshold = 1e-5,minibatch_size=64,n_epochs=15,learning_rate_choice=1,learning_rate=0.025,grad_descent_choice = 1,regularization = 0,reg_lambda =8e-4, noise = True,noise_stdev = 0.25e-2): #class constructor which initializes all the parameters required to fire the MLP\n",
    "            self.architecture     = architecture  #list containing the number of neurons in each layer so here it'll be [h1,h2,h3]\n",
    "            self.no_layers        = len(architecture)+1 #not considering the input layer\n",
    "            self.activations      = activations   #activation function for each layer\n",
    "            self.train_data       = train_images\n",
    "            self.train_labels     = train_labels\n",
    "            self.minibatch_size   = minibatch_size\n",
    "            self.n_epochs         = n_epochs\n",
    "            self.lr_choice        = learning_rate_choice\n",
    "            self.learning_rate    = learning_rate\n",
    "            self.grad_descent     = grad_descent_choice\n",
    "            self.input_dim        = len(train_images)\n",
    "            self.train_nos        = len(train_images[0])\n",
    "            self.output_dim       = len(train_labels)\n",
    "            self.test_data        = test_images\n",
    "            self.test_labels      = test_labels\n",
    "            self.test_nos         = len(test_images[0])\n",
    "            self.weights          = {}\n",
    "            self.biases           = {}\n",
    "            self.inactive_neurons = {}\n",
    "            self.thresh           = threshold\n",
    "            self.test_pred        = []\n",
    "            self.train_losses     = []\n",
    "            self.test_losses      = []\n",
    "            self.train_accuracy   = []\n",
    "            self.test_accuracy    = [] \n",
    "            self.plot             = plot\n",
    "            self.plot_mode        = plot_mode\n",
    "            self.reg              = regularization\n",
    "            self.reg_lambda       = reg_lambda\n",
    "            self.noise            = noise\n",
    "            self.noise_stdev      = noise_stdev\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(self.no_layers): #initialising the dictionaries\n",
    "                self.weights[i]          = []\n",
    "                self.biases[i]           = []\n",
    "                self.inactive_neurons[i] = []\n",
    "                \n",
    "    def glorot_initialization(self,fan_in,fan_out):\n",
    "        dl = np.sqrt(6/(fan_in+fan_out))\n",
    "        return np.asarray(np.random.uniform(-dl,dl,(fan_out,fan_in)),dtype = np.float64)\n",
    "    \n",
    "    def initialize_weights_biases(self):\n",
    "        \n",
    "        for i in range(self.no_layers):\n",
    "            \n",
    "            if(i==0):\n",
    "                self.weights[i] = self.glorot_initialization(self.input_dim,self.architecture[0])\n",
    "                self.biases[i]  = np.zeros((self.architecture[0],1),dtype = np.float64)\n",
    "                #print(self.weights[i].shape,self.biases[i].shape)\n",
    "            \n",
    "            elif(i== len(self.architecture)):\n",
    "                self.weights[i] = self.glorot_initialization(self.architecture[-1],self.output_dim)\n",
    "                self.biases[i]  = np.zeros((self.output_dim,1),dtype = np.float64)\n",
    "                #print(self.weights[i].shape,self.biases[i].shape)\n",
    "            \n",
    "            else:\n",
    "                self.weights[i] = self.glorot_initialization(self.architecture[i-1],self.architecture[i])\n",
    "                self.biases[i]  = np.zeros((self.architecture[i],1),dtype = np.float64)\n",
    "                #print(self.weights[i].shape,self.biases[i].shape)\n",
    "                \n",
    "    \n",
    "    def forward_pass(self,data): #data is a matrix with dimensions: input_dim x minibatch_size\n",
    "        z_vals = {}\n",
    "        a_vals = {}\n",
    "        q_vals = np.zeros((self.output_dim,len(data)))\n",
    "        for i in range(self.no_layers):\n",
    "            if(i == 0):\n",
    "                #print(self.biases[0].shape,self.weights[0].shape,data.shape)\n",
    "                z_vals[i] = np.dot(self.weights[0],data) + np.dot(self.biases[0],np.ones((1,len(data[0])))) #broadcasting results in an index error for last iter of epoch so do it explicitly\n",
    "                #print(z_vals[i].shape)\n",
    "                a_vals[i] = activation_library[self.activations[i]](z_vals[i])\n",
    "                #print(\"zvals shape,avals shape\",i,\":\",z_vals[i].shape,a_vals[i].shape)\n",
    "            \n",
    "            else:\n",
    "                z_vals[i] = np.dot(self.weights[i],a_vals[i-1]) + np.dot(self.biases[i],np.ones((1,len(data[0]))))\n",
    "                a_vals[i] = activation_library[self.activations[i]](z_vals[i])\n",
    "                #print(\"zvals shape,avals shape\",i,\":\",z_vals[i].shape,a_vals[i].shape)\n",
    "            \n",
    "            if(i == len(self.architecture)): #output values\n",
    "                #q_vals = softmax(a_vals[i])\n",
    "                q_vals = activation_library[self.activations[-1]](a_vals[i]) #final activation \n",
    "                #print(q_vals.shape)\n",
    "        return z_vals,a_vals,q_vals\n",
    "    \n",
    "    \n",
    "    def backprop(self,z_vals,a_vals,q_vals,data_labels,data):\n",
    "        deltas = {}\n",
    "        weight_gradients = {}\n",
    "        bias_gradients = {}\n",
    "        \n",
    "        for i in range(len(self.architecture),-1,-1): #performs the backprop across all layers starting from the final layer\n",
    "            \n",
    "            if(i == len(self.architecture)):\n",
    "                deltas[i] = activation_derivative_library[self.activations[-1]](data_labels,q_vals)   #delta index starts from 4 instead of 5 in class\n",
    "            else:\n",
    "                deltas[i] = np.multiply(np.dot(np.transpose(self.weights[i+1]),deltas[i+1]),activation_derivative_library[self.activations[i]](z_vals[i])) #element wise product np.multiply\n",
    "            \n",
    "            self.inactive_neurons[i].append(np.sum((deltas[i] < self.thresh)*1)/(deltas[i].shape[0]*deltas[i].shape[1]))    \n",
    "            if(i==0):\n",
    "                #print(deltas[i].shape,np.transpose(data).shape)\n",
    "                weight_gradients[i] = np.dot(deltas[i],np.transpose(data))\n",
    "            else:\n",
    "                #print(deltas[i].shape,np.transpose(a_vals[i-1]).shape)\n",
    "                weight_gradients[i] = np.dot(deltas[i],np.transpose(a_vals[i-1]))\n",
    "                \n",
    "            bias_gradients[i] = deltas[i]\n",
    "        \n",
    "        return weight_gradients,bias_gradients\n",
    "    \n",
    "        \n",
    "    \n",
    "    def train_MLP(self):\n",
    "        N_batches = int(len(self.train_data[0])//self.minibatch_size + 1)\n",
    "        self.initialize_weights_biases()\n",
    "        if(self.reg == 2):\n",
    "            const = 1-(self.reg_lambda/self.minibatch_size)\n",
    "            reg1 = False\n",
    "        else:\n",
    "            if(self.reg == 1):\n",
    "                reg1 = True\n",
    "            else:\n",
    "                reg1 = False\n",
    "            const      = 1 #constant\n",
    "        train_loss = []\n",
    "        test_loss  = []\n",
    "        iterations = []\n",
    "        r_w          = [0]*self.no_layers\n",
    "        s_w          = [0]*self.no_layers\n",
    "        r_b          = [0]*self.no_layers\n",
    "        s_b          = [0]*self.no_layers\n",
    "    \n",
    "        for e in range(self.n_epochs):\n",
    "            print(\"Epoch\",e,\"is training\")\n",
    "            #shuffling the dataset for every epoch\n",
    "            train_data_epoch,train_labels_epoch = sklearn.utils.shuffle(self.train_data.T,self.train_labels.T)\n",
    "            train_data_epoch   = train_data_epoch.T\n",
    "            if(self.noise == True):\n",
    "                train_data_epoch += np.random.normal(0, self.noise_stdev, np.shape(train_data_epoch))\n",
    "            train_labels_epoch = train_labels_epoch.T\n",
    "            \n",
    "            for n in tqdm(range(N_batches)):\n",
    "            #for n in range(N_batches):\n",
    "                if(n == N_batches-1):\n",
    "                    #data        = self.train_data[:,n*self.minibatch_size:]\n",
    "                    #data_labels = self.train_labels[:,n*self.minibatch_size:]\n",
    "                    data        = train_data_epoch[:,n*self.minibatch_size:]\n",
    "                    data_labels = train_labels_epoch[:,n*self.minibatch_size:]\n",
    "                \n",
    "                else:\n",
    "                    data        = train_data_epoch[:,n*self.minibatch_size:(n+1)*self.minibatch_size]\n",
    "                    data_labels = train_labels_epoch[:,n*self.minibatch_size:(n+1)*self.minibatch_size]\n",
    "                    \n",
    "                \n",
    "                \n",
    "                z_vals,a_vals,q_vals = self.forward_pass(data)\n",
    "                \n",
    "                weight_gradients,bias_gradients = self.backprop(z_vals,a_vals,q_vals,data_labels,data)\n",
    "                \n",
    "                for i in range(self.no_layers):\n",
    "                    # (learning_rate,theta,minibatch_size,theta_grad,t,r_t = 0,s_t = 0,delta = 10e-10,pho_1 = 0.9,pho_2 = 0.85,bias = True)\n",
    "                    if((e == 0) and (n == 0)):    \n",
    "                        self.weights[i],r_w[i],s_w[i] = grad_descent_library[self.grad_descent](learning_rate_library[self.lr_choice](self.learning_rate,e),const*self.weights[i],self.minibatch_size,weight_gradients[i],e*N_batches+n,bias = False,reg1 = reg1)\n",
    "                        self.biases[i],r_b[i],s_b[i]  = grad_descent_library[self.grad_descent](learning_rate_library[self.lr_choice](self.learning_rate,e),self.biases[i],self.minibatch_size,bias_gradients[i],e*N_batches+n)  \n",
    "                        r_w[i] = np.asarray(r_w[i])\n",
    "                        r_b[i] = np.asarray(r_b[i])\n",
    "                        s_w[i] = np.asarray(s_w[i])\n",
    "                        s_b[i] = np.asarray(s_b[i])\n",
    "                    else:\n",
    "                        self.weights[i],r_w[i],s_w[i] = grad_descent_library[self.grad_descent](learning_rate_library[self.lr_choice](self.learning_rate,e),const*self.weights[i],self.minibatch_size,weight_gradients[i],e*N_batches+n,r_w[i],s_w[i],bias = False,reg1 = reg1)\n",
    "                        self.biases[i],r_b[i],s_b[i]  = grad_descent_library[self.grad_descent](learning_rate_library[self.lr_choice](self.learning_rate,e),self.biases[i],self.minibatch_size,bias_gradients[i],e*N_batches+n,r_b[i],s_b[i])                                       \n",
    "                        r_w[i] = np.asarray(r_w[i])\n",
    "                        r_b[i] = np.asarray(r_b[i])\n",
    "                        s_w[i] = np.asarray(s_w[i])\n",
    "                        s_b[i] = np.asarray(s_b[i])\n",
    "                \n",
    "                if(self.reg):\n",
    "                    train_loss.append(reg_loss[self.reg](data_labels,q_vals,self.weights,self.minibatch_size))\n",
    "                else:\n",
    "                    train_loss.append(cross_entropy_loss(data_labels,q_vals))\n",
    "                \n",
    "                if(((N_batches*e+n)%200 == 0)):#every 200 iterations\n",
    "                    q_test = self.forward_pass(self.test_data)[-1]\n",
    "                    #print(q_test.shape)\n",
    "                    test_loss.append(cross_entropy_loss(self.test_labels,q_test))\n",
    "                    iterations.append(N_batches*e+n)             \n",
    "                   \n",
    "        self.test_losses  = test_loss           \n",
    "        self.train_losses = train_loss\n",
    "        self.test_pred    = self.forward_pass(self.test_data)[-1]\n",
    "        \n",
    "        if(self.plot):\n",
    "            self.basic_plots(iterations,self.plot_mode)\n",
    "            \n",
    "        \n",
    "        #return test_loss,train_loss\n",
    "    \n",
    "    \n",
    "    def basic_plots(self,iter_list,Mode = 0):\n",
    "        \n",
    "        #first plotting the training and the testing losses\n",
    "        fig, (ax1,ax2) =plt.subplots(2,sharex = True)\n",
    "        \n",
    "        ax1.plot(self.train_losses,'o-')\n",
    "        ax1.set_title(\"Average Training Losses vs iterations\")\n",
    "        ax1.grid(True)\n",
    "        ax1.set_ylabel(\"Training loss\")\n",
    "        \n",
    "        ax2.plot(iter_list,self.test_losses,'+-')\n",
    "        ax2.set_title(\"Average Testing Losses vs iterations\")\n",
    "        ax2.grid(True)\n",
    "        ax2.set_ylabel(\"Testing loss\")\n",
    "        ax2.set_xlabel(\"<- Iterations ->\")\n",
    "        \n",
    "        fig.set_figwidth(10)\n",
    "        fig.set_figheight(8)\n",
    "        fig.show()\n",
    "        accuracy,mean,std = basic_stats(self.test_pred,self.test_labels)\n",
    "        print(\"Accuracy:\",accuracy)\n",
    "        print(\"Mean of errors:\",mean)\n",
    "        print(\"Standard deviation of errors:\",std)\n",
    "        if(Mode != 0):\n",
    "            \n",
    "            #plotting the percentage of inactive neurons\n",
    "            fig,ax = plt.subplots()\n",
    "            for i in range(self.no_layers):\n",
    "                ax.plot(self.inactive_neurons[i],label = \"Neurons at layer\"+str(i+1))\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.legend(loc = 'best')\n",
    "            plt.xlabel('<-Iterations ->')\n",
    "            plt.ylabel('% of Inactive neurons')\n",
    "            fig.suptitle('Percentage of Inactive neurons present in every layer')\n",
    "            plt.show()\n",
    "\n",
    "            #plotting the confusion_matrix for both train and test\n",
    "            #cm_1,p_1,r_1,f1_1 = more_stats(self.train_pred,self.train_labels)\n",
    "            cm_2,p_2,r_2,f1_2 = more_stats(self.test_pred,self.test_labels)\n",
    "            \n",
    "            '''\n",
    "            plt.imshow(cm_1, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "            classNames = ['0','1','2','3','4','5','6','7','8','9']\n",
    "            plt.title('Train Data: Confusion Matrix')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            tick_marks = np.arange(len(classNames))\n",
    "            plt.xticks(tick_marks, classNames)\n",
    "            plt.yticks(tick_marks, classNames)\n",
    "\n",
    "            for i in range(len(classNames)):\n",
    "                for j in range(len(classNames)):\n",
    "                    plt.text(j,i,str(cm_1[i][j]))\n",
    "            plt.show()\n",
    "            '''\n",
    "                   \n",
    "            plt.imshow(cm_2, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "            plt.colorbar()\n",
    "            classNames = ['0','1','2','3','4','5','6','7','8','9']\n",
    "            plt.title('Test Data: Confusion Matrix')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            tick_marks = np.arange(len(classNames))\n",
    "            plt.xticks(tick_marks, classNames)\n",
    "            plt.yticks(tick_marks, classNames)\n",
    "            \n",
    "            #for i in range(len(classNames)):\n",
    "            #    for j in range(len(classNames)):\n",
    "            #        plt.text(j,i,str(cm_2[i][j]))\n",
    "            plt.show()\n",
    "            \n",
    "            '''       \n",
    "            #precision, recall plots\n",
    "            fig, ax = plt.subplots()\n",
    "            index = np.arange(n_groups)\n",
    "            bar_width = 0.25\n",
    "            opacity = 0.8\n",
    "\n",
    "            rects1 = plt.bar(index, p_1, bar_width,\n",
    "            alpha=opacity,\n",
    "            color='b',\n",
    "            label='Precision')\n",
    "\n",
    "            rects2 = plt.bar(index + bar_width, r_1, bar_width,\n",
    "            alpha=opacity,\n",
    "            color='g',\n",
    "            label='Recall')\n",
    "\n",
    "            rects3 = plt.bar(index + 2*bar_width,f1_1,bar_width,alpha=opacity,color = 'r',label=\"F1 Score\")\n",
    "\n",
    "            plt.xlabel('Digits')\n",
    "            plt.ylabel('Scores')\n",
    "            plt.title('Train Data: Precision, Recall and F1 score')\n",
    "            plt.xticks(index + 1.5*bar_width, classNames)\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            '''\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            index = np.arange(len(p_2))\n",
    "            bar_width = 0.25\n",
    "            opacity = 0.8\n",
    "\n",
    "            rects1 = plt.bar(index, p_2, bar_width,\n",
    "            alpha=opacity,\n",
    "            color='b',\n",
    "            label='Precision')\n",
    "\n",
    "            rects2 = plt.bar(index + bar_width, r_2, bar_width,\n",
    "            alpha=opacity,\n",
    "            color='g',\n",
    "            label='Recall')\n",
    "\n",
    "            rects3 = plt.bar(index + 2*bar_width,f1_2,bar_width,alpha=opacity,color = 'r',label=\"F1 Score\")\n",
    "\n",
    "            plt.xlabel('Digits')\n",
    "            plt.ylabel('Scores')\n",
    "            plt.title('Test Data: Precision, Recall and F1 score')\n",
    "            plt.xticks(index + 1.5*bar_width, classNames)\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "                    \n",
    "                \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "        \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main script for the assignment\n",
    "\n",
    "#question 1\n",
    "#experimenting with learning rates\n",
    "grid_of_learning_rates = np.asarray([0.01,0.02,0.025,0.03,0.04,0.05])\n",
    "\n",
    "for learning_rate in tqdm(grid_of_learning_rates):\n",
    "    NN_sigmoid = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=0,learning_rate = learning_rate,noise= False)\n",
    "    NN_sigmoid.train_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_of_learning_rates = np.asarray([0.06,0.07,0.08,0.09,0.1])\n",
    "for learning_rate in tqdm(grid_of_learning_rates):\n",
    "    NN_sigmoid = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=0,learning_rate = learning_rate,noise= False)\n",
    "    NN_sigmoid.train_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimenting with gradient descent algorithms\n",
    "\n",
    "NN_sigmoid = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=0,learning_rate_choice = 2,learning_rate = 0.025,noise= False)\n",
    "NN_sigmoid.train_MLP()\n",
    "\n",
    "for i in range(4):\n",
    "    NN_sigmoid = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=0,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = i+1,noise= False)\n",
    "    NN_sigmoid.train_MLP()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing the various activation functions\n",
    "\n",
    "NN_sigmoid = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4,noise= False)\n",
    "NN_sigmoid.train_MLP()\n",
    "\n",
    "NN_relu   = MLP([500,250,100],[2,2,2,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 1,noise= False)\n",
    "NN_relu.train_MLP()\n",
    "\n",
    "NN_tanh  = MLP([500,250,100],[3,3,3,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4,noise= False)\n",
    "NN_tanh.train_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization with a very high value\n",
    "NN_sigmoid_r1 = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,n_epochs=15,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4,regularization=1,noise= False)\n",
    "NN_sigmoid_r1.train_MLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization and augmentation\n",
    "NN_sigmoid_r1 = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4,regularization=1,noise= False)\n",
    "NN_sigmoid_r1.train_MLP()\n",
    "\n",
    "NN_sigmoid_r2 = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4,regularization=2,noise= False)\n",
    "NN_sigmoid_r2.train_MLP()\n",
    "\n",
    "NN_sigmoid_n = MLP([500,250,100],[1,1,1,4,5],train_data,one_hot_train,test_data,one_hot_test,plot_mode=1,learning_rate_choice = 1,learning_rate = 0.025,grad_descent_choice = 4)\n",
    "NN_sigmoid_n.train_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NN_sigmoid_r1.weights[2], interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "plt.colorbar()\n",
    "plt.title('Weights for L1 regularization with high lambda')\n",
    "plt.ylabel('Second Hidden Layer')\n",
    "plt.xlabel('Third Hidden layer')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(NN_sigmoid_r2.weights[2], interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "plt.colorbar()\n",
    "plt.title('Weights for L2 regularization')\n",
    "plt.ylabel('Second Hidden Layer')\n",
    "plt.xlabel('Third Hidden layer')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(NN_sigmoid_n.weights[2], interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "plt.colorbar()\n",
    "plt.title('Weights for noise addition')\n",
    "plt.ylabel('Second Hidden Layer')\n",
    "plt.xlabel('Third Hidden layer')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(np.sum((NN_sigmoid.weights[2] > NN_sigmoid_r2.weights[2] )*1)/(NN_sigmoid_r1.weights[2].shape[0]*NN_sigmoid_r1.weights[2].shape[1]))\n",
    "print(np.sum((NN_sigmoid.weights[2] > NN_sigmoid_n.weights[2] )*1)/(NN_sigmoid_r1.weights[2].shape[0]*NN_sigmoid_r1.weights[2].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction part, run in collab\n",
    "import cv2\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "\n",
    "def deskew(img, imgSize):\n",
    "    # calculate image moments\n",
    "    m = cv2.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        # no deskewing needed\n",
    "        return img.copy()\n",
    "\n",
    "    # calculate skew based on central moments\n",
    "    skew = m['mu11'] / m['mu02']\n",
    "\n",
    "    # calculate affine transformation to correct skewness\n",
    "    M = np.float32([[1, skew, -0.5*imgSize*skew], [0, 1, 0]])\n",
    "\n",
    "    # apply affine transformation\n",
    "    img = cv2.warpAffine(img, M, (imgSize, imgSize), flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Load the mnist dataset\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "\n",
    "imsize = 28 # size of image (28x28)\n",
    "\n",
    "# HOG parameters:\n",
    "winSize = (imsize, imsize) # 28, 28\n",
    "blockSize = (imsize//2, imsize//2) # 14, 14    \n",
    "cellSize = (imsize//2, imsize//2) #14, 14\n",
    "blockStride = (imsize//4, imsize//4) # 7, 7\n",
    "nbins = 9\n",
    "signedGradients = True\n",
    "derivAperture = 1\n",
    "winSigma = -1.0\n",
    "histogramNormType = 0\n",
    "L2HysThreshold = 0.2\n",
    "gammaCorrection = 1\n",
    "nlevels = 64\n",
    "\n",
    "# define the HOG descriptor\n",
    "hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins, derivAperture, winSigma, \n",
    "                        histogramNormType, L2HysThreshold, gammaCorrection, nlevels, signedGradients)\n",
    "\n",
    "# compute HOG descriptors\n",
    "train_values = []\n",
    "for i in range(trainX.shape[0]):\n",
    "    trainX[i] = deskew(trainX[i], 28) # deskew the current image\n",
    "    train_hog = hog.compute(trainX[i]) # compute the HOG features\n",
    "    train_values.append(train_hog) # append it to the train values list\n",
    "\n",
    "test_values = []\n",
    "for i in range(testX.shape[0]):\n",
    "    testX[i] = deskew(testX[i], 28) # deskew the current image\n",
    "    test_hog = hog.compute(testX[i]) # compute the HOG features\n",
    "    test_values.append(test_hog) # append it to the test values list\n",
    "\n",
    "\n",
    "train_values = np.resize(train_values, (trainX.shape[0], 81))\n",
    "\n",
    "\n",
    "test_values = np.resize(test_values, (testX.shape[0], 81))\n",
    "\n",
    "# classifier\n",
    "clf = svm.SVC(C=1.0, kernel='rbf') #try linear also, same output is obtained 98.5 accuracy\n",
    "clf2 = knn(n_neighbors=200) #about 96 accuracy\n",
    "clf2.fit(train_values,trainY)\n",
    "clf.fit(train_values, trainY)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(testY, clf.predict(test_values)))\n",
    "print(classification_report(testY,clf2.predict(test_values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_relu = MLP([500,250,100],[2,2,2,4,5],np.asarray(train_values).T,one_hot_train,np.asarray(test_values).T,one_hot_test,plot_mode=1,n_epochs = 25,learning_rate = 0.0025,grad_descent_choice = 4)\n",
    "NN_relu.train_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler1 = StandardScaler()\n",
    "trainX = trainX.reshape(5000,784)#if you're using 5000 samples\n",
    "testX  = testX.reshape(5000,784)#if you're using 5000 samples\n",
    "scaler1.fit(trainX)\n",
    "train = scaler1.transform(trainX)\n",
    "scaler1.fit(testX)\n",
    "test  = scaler1.transform(testX)\n",
    "\n",
    "pca1 = PCA(n_components = 81)\n",
    "pca1.fit(train)\n",
    "train_pca = pca1.transform(train)\n",
    "pca1.fit(test)\n",
    "test_pca = pca1.transform(test)\n",
    "\n",
    "print(test_pca.shape,train_pca.shape)\n",
    "\n",
    "clf = svm.SVC(C=1.0, kernel='rbf')\n",
    "clf2 = knn(n_neighbors=30)\n",
    "clf2.fit(train_pca,trainY)\n",
    "clf.fit(train_pca, trainY)\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(testY, clf.predict(test_pca)))\n",
    "print(classification_report(testY,clf2.predict(test_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
